{
    "bert_config": {
        "hidden_size": 768,
        "vocab_size": 30400,
        "num_attention_heads": 12,
        "num_hidden_layers": 12,
        "intermediate_size": 3072,
        "hidden_dropout_prob": 0.1,
        "attention_probs_dropout_prob": 0.1,
        "max_position_embeddings": 512,
        "type_vocab_size": 2,
        "initializer_range": 0.02,
        "layer_norm_eps": 1e-12,
        "position_embedding_type": "absolute"
    },
    "seed": 1984,
    "replicas": 1,
    "micro_batch_size": 4,
    "grad_acc_steps_per_replica": 80,
    "num_epochs": 3,
    "optimizer_opts": {
        "name": "LAMB",
        "params": {
            "weight_decay_rate": 0.01
        }
    },
    "learning_rate": {
        "lr_schedule": "up_down",
        "lr_schedule_params": {
            "max_learning_rate": 1e-3,
            "warmup_frac": 0.3
        }
    },
    "replace_layers": true,
    "use_outlining": true,
    "enable_recomputation": true,
    "optimizer_state_offchip": true,
    "matmul_available_memory_proportion_per_pipeline_stage": [0.25, 0.23, 0.28, 0.25],
    "matmul_partials_type": "half",
    "embedding_serialization_factor": 4,
    "pipeline_stages": [
         ["emb"],
         ["hid","hid", "hid", "hid"],
         ["hid","hid", "hid", "hid"],
         ["hid","hid", "hid", "hid"],
         ["qa_head"]
        ],
    "device_mapping": [0, 1, 2, 3, 0],
    "global_batches_per_log": 5,
    "wandb_opts": {
        "log_to_wandb": true,
        "init": {
            "project": "tf2-bert",
            "notes": "",
            "tags": ["squad_fine_tuning"],
            "name": "base"
        }
    },
    "cache_dir": "./cache/",
    "output_dir": "./output_dir_hf/"
}
