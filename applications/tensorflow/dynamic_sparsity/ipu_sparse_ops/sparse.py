# Copyright (c) 2020 Graphcore Ltd. All rights reserved.

import json
import os

import math
import random
import numpy as np
import tensorflow.compat.v1 as tf

from ipu_sparse_ops import host_utils
from logging import getLogger
from tensorflow.python import ipu
from tensorflow.python.ipu.config import (
    DeviceConnectionType,
    IPUConfig
)
from typing import (
    Callable,
    List,
    NamedTuple,
    Optional,
    Tuple,
    Union
)

tf.disable_eager_execution()
tf.disable_v2_behavior()

logger = getLogger(os.path.basename(__file__))

host_utils.initialize_device_manager()


def get_lib_path(lib_name):
    base_path = os.path.realpath(os.path.dirname(__file__))
    return os.path.join(base_path, "lib" + lib_name + ".so")


class MatmulSpec(NamedTuple):
    """
    Named tuple that specifies a sparse by dense matmul.

    max_non_zero_blocks - The maximum number of non-zero blocks.
    block_size - Size of the non-zero blocks. Blocks are square, non-overlapping,
                 and must divide equally into the sparse matrix rows and columns.
    num_groups - Number of grouped sparse matrices to apply to the dense input.
    batch_size - Number of vectors in the dense input.
    input_size - Dimension of vectors in the dense input.
    output_size - Dimension of vectors in the dense output.
    data_type - tf.DType - Data type for the sparse matrix.
    pooling_type - Dense gradient pooling per block type among
    [NONE, SUM, AVG, MAX]. None disables pooling,
    SUM enables sum pooling, AVG enables average pooling,
    and MAX enables max pooling. If block_size is 1, pooling is disabled.
    """
    max_non_zero_blocks: int
    block_size: int
    num_groups: int
    batch_size: int
    input_size: int
    output_size: int
    data_type: tf.DType
    pooling_type: str


class Triplets(NamedTuple):
    """
    Named tuple storing a (block) COO representation of a sparse matrix.
    In the case that the structure is block-sparse rather than elements/scalars
    then indices refer to block coordinates and values is a list of tensors
    (square blocks in the matrix).

    Blocks must be stored in major-minor order (row major).

    row_indices - list of (block) row coordinates for the non-zeros.
    col_indices - list of (block) column coordinates for the non-zeros.
    values - list of (block) values.
    """
    row_indices: List[int]
    col_indices: List[int]
    values: List[float]


class TripletStats(NamedTuple):
    """
    Stores statistics for triplets.
    """
    row_range: Tuple
    col_range: Tuple
    value_range: Tuple
    median_row_index: int
    median_col_index: int
    median_value: float


class SparseRepresentation:
    """
    This class stores variables that hold the host-side representation
    of a sparse weight matrix in the IPU's native format (metainfo
    and non-zero values).
    """
    def __init__(self, metainfo: List[np.uint16], nz_values: List[np.float]):
        self.metainfo_state = metainfo
        self.nz_values = nz_values
        self.metainfo_state_fp16 = self.metainfo_state.view(dtype=np.float16)

    def makePlaceHolders(self, data_type) -> Tuple[tf.Tensor, tf.Tensor]:
        metainfo_ph = tf.placeholder(tf.float16, self.metaInfoShape())
        nz_ph = tf.placeholder(data_type, self.valuesShape())
        return metainfo_ph, nz_ph

    def metaInfoShape(self) -> List[int]:
        return [self.metainfo_state.size]

    def valuesShape(self) -> List[int]:
        return [self.nz_values.size]

    def metaInfoFeed(self) -> np.ndarray:
        # XLA requires us to pass only floating point tensors to custom ops:
        return self.metainfo_state_fp16

    def valuesFeed(self) -> np.ndarray:
        return self.nz_values

    def __str__(self):
        return f"metainfo: {self.metainfo_state} values:{self.nz_values}"


class _SystemConfig(NamedTuple):
    """Configuration carrying the configuration for host_utils/sparse_matmul.

    ipu_version: int - If non-zero determines which ipu version to target
                       without looking up available devices.
    custom_op_debug: bool - Whether to print debug message each time dense
                            gradient is calculated.
    """
    ipu_version: int = 0
    custom_op_debug: bool = False


_GLOBAL_SYSTEM_CONFIG = _SystemConfig()


def set_system_config(opts: IPUConfig, custom_op_debug_printing=False):
    """Set the underlying device configuration from the TF IPU config.
    Returns the config unmodified.

    :param opts: The ipu options config generated by ipu.utils
    :param custom_op_debug_printing: Enable dense gradient debug printing
    """
    global _GLOBAL_SYSTEM_CONFIG

    ipu_version_int = {
        '': 0,
        'ipu1': 1,
        'ipu2': 2
    }.get(opts.device_connection.version, None)

    if ipu_version_int is None:
        raise ValueError(f'Unrecognised device_connection.version: \'{opts.device_connection.version}\'')

    _GLOBAL_SYSTEM_CONFIG = _SystemConfig(
        ipu_version=(
            0
            if opts.device_connection.type is not DeviceConnectionType.NEVER else
            ipu_version_int),
        custom_op_debug=custom_op_debug_printing
    )

    logger.info(f"Device config set with ipu_version={_GLOBAL_SYSTEM_CONFIG.ipu_version} "
                f"custom_op_debug={_GLOBAL_SYSTEM_CONFIG.custom_op_debug}")
    return opts


def concatenate_triplets(a: Triplets, b: Triplets, dim_a: List[int], axis: int = 0) -> Triplets:
    """
    Concatenate 2 sparse matrices from their triplets representation.
    Returns the triplets representation of the concatenated matrices.
    :param a: The first matrix to concatenate, as a triplets object
    :param b: The second matrix to concatenate, as a triplets object
    :param dim_a: The dimension of the first matrix. Must be an array of length 2
    :param axis: The axis along which to concatenate (0 means contatenate along rows)
    """
    if len(dim_a) != 2 or axis > 1 or axis < 0:
        raise Exception("Sparse triplets can only represent 2D matrices")

    b_rows = b.row_indices if axis == 1 else np.array([index + dim_a[0] for index in b.row_indices])
    b_cols = b.col_indices if axis == 0 else np.array([index + dim_a[1] for index in b.col_indices])

    rows = np.concatenate((a.row_indices, b_rows))
    cols = np.concatenate((a.col_indices, b_cols))
    values = np.concatenate((a.values, b.values))
    return Triplets(rows, cols, values)


def split_triplets(triplets: Triplets, split_index: Union[int, List[int]], axis: int = 0) -> List[Triplets]:
    """
    Split a sparse matrix from its triplets representation into two
    (Also represented as triplets).
    :param triplets: The matrix to split, as a triplets object
    :param split_index: The matrix index where to split, i.e. the size of the left hand matrix along the split axis.
    Multiple indices can be passed in a list for multiple splits.
    :param axis: Axis along which to split. (0 means along the rows)
    e.g. a if split_index = 2, axis=0, [shape(4,2)] -> [shape(2,2)] , [shape(2,2)]
    """
    if (isinstance(split_index, int)):
        split_index = [split_index]
    elif (isinstance(split_index, list)):
        split_index.sort()
    else:
        raise Exception("split_index must be an integer or a list of integers")

    if axis == 0:
        index_list = np.array(triplets.row_indices)
    elif axis == 1:
        index_list = np.array(triplets.col_indices)
    else:
        raise Exception("Cannot split along required dimension (non-existant).")

    split = split_index[0]
    a_index = []
    b_index = []
    for k in range(index_list.size):
        if index_list[k] < split: a_index.append(k)
        else: b_index.append(k)

    a_rows = np.array([triplets.row_indices[i] for i in a_index], dtype=np.uint64)
    a_cols = np.array([triplets.col_indices[i] for i in a_index], dtype=np.uint64)
    a_vals = np.array([triplets.values[i] for i in a_index])

    b_rows = np.array([triplets.row_indices[i] for i in b_index], dtype=np.uint64)
    b_cols = np.array([triplets.col_indices[i] for i in b_index], dtype=np.uint64)
    b_vals = np.array([triplets.values[i] for i in b_index])

    # Remove the indices offset
    if axis == 0:
        b_rows = np.array([i - split for i in b_rows], dtype=np.uint64)
    elif axis == 1:
        b_cols = np.array([i - split for i in b_cols], dtype=np.uint64)
    split_index = [i - split for i in split_index]

    if len(split_index) == 1:
        return [Triplets(a_rows, a_cols, a_vals), Triplets(b_rows, b_cols, b_vals)]
    else:
        remaining_triplets = split_triplets(Triplets(b_rows, b_cols, b_vals), split_index[1:], axis)
        return [Triplets(a_rows, a_cols, a_vals)] + remaining_triplets


def check_shape(shape: List[int], block_size: int, max_non_zeros: int):
    not_divisible = [s % block_size != 0 for s in shape]
    if any(not_divisible):
        raise ValueError(f"Every element of shape {shape} must be divisible by the block size {block_size}")
    total_blocks = np.prod([s // block_size for s in shape])
    if max_non_zeros > total_blocks:
        raise ValueError(f"More non-zeros ({max_non_zeros}) than there are blocks ({total_blocks})")


def matmul_spec_from_density(hidden_size: int, input_shape: List[int],
                             density: float, block_size: int, dtype: tf.DType,
                             pooling_type: str = 'NONE') -> MatmulSpec:
    """
    Utility to build a sparse matrix multiply specification object by specifying the density
    proportion of the layer (where density = 1 - sparsity).
    """
    num_blocks = (input_shape[1] // block_size) * (hidden_size // block_size)
    max_non_zeros = int(np.ceil(density * num_blocks))
    check_shape((input_shape[1], hidden_size), block_size, max_non_zeros)
    return MatmulSpec(
        block_size=block_size,
        input_size=input_shape[1], output_size=hidden_size,
        num_groups=1, batch_size=input_shape[0],
        data_type=dtype,
        max_non_zero_blocks=max_non_zeros,
        pooling_type=pooling_type)


def matmul_spec_from_max(hidden_size: int, input_shape: list,
                         max_non_zeros: int, block_size: int, dtype: tf.DType,
                         pooling_type: str = 'NONE') -> MatmulSpec:
    """
    Utility to build a sparse matrix multiply specification object by specifying the maxumim
    number of non zero entries.
    """
    check_shape((input_shape[1], hidden_size), block_size, max_non_zeros)
    return MatmulSpec(
        block_size=block_size,
        input_size=input_shape[1], output_size=hidden_size,
        num_groups=1, batch_size=input_shape[0],
        data_type=dtype,
        max_non_zero_blocks=max_non_zeros,
        pooling_type=pooling_type)


def block_shape_from_list(values: List):
    """
    Get the block shape of a list by converting
    it to a numpy array and taking the postfix
    of its numpy shape.
    """
    shape = np.array(values).shape
    if shape == 1 or len(shape) <= 1:
        return (1, 1)
    return shape[1:]


def block_size_from_list(values: List):
    """
    Return the shape of blocks in the list. Assumes the shape
    without the first index describes the block shape and that
    the blocks are square. If the input is a list or vector
    there is no way to infer a block size so it will be assumed
    to be 1.
    """
    shape = block_shape_from_list(values)
    # If the shape is a list/vector the block size can only be 1:
    if len(shape) == 1:
        if shape[0] == 1 or shape[0] is None:
            return 1
        else:
            raise ValueError("Blocks must be square to infer size.")
    # Do not currently support block tensors higher than matrices:
    if len(shape) != 2:
        raise ValueError(f"Block has order {len(shape)}: only block sizes 1 and 2 are supported.")
    # Check blocks are square:
    if shape[0] != shape[1]:
        raise ValueError("Blocks must be square to infer size.")
    return shape[0]


def unflatten_blocks(spec: MatmulSpec, values: List[float]) -> np.ndarray:
    """
    Reshape a flat list of values into the correct block-shape for the given spec.
    This is the inverse of :func:`flatten_blocks`.
    :param spec: Matrix specification that contains the correct block size.
                 If block size is 1 then no reshape is performed and if
                 the list is not flat then no reshape is performed (assume
                 it is already correct shape).
    :param values: The flat list of values to reshape into blocks.
    """
    n_values = len(values)
    if spec.block_size == 1 or len(np.array(values).shape) != 1:
        if n_values > spec.max_non_zero_blocks:
            raise ValueError(f"Number of values {n_values} exceeds max {spec.max_non_zero_blocks}")
        return values
    elements = n_values // (spec.block_size * spec.block_size)
    if elements > spec.max_non_zero_blocks:
        raise ValueError(f"Number of bloks {elements} exceeds max {spec.max_non_zero_blocks}")
    return np.transpose(np.reshape(values, [elements, spec.block_size, spec.block_size]), (0, 2, 1))


def flatten_blocks(spec: MatmulSpec, block_values: np.ndarray) -> np.ndarray:
    """
    Reshape block values into flat list in the order that host_utils expects.
    This is the inverse of :func:`unflatten_blocks`.
        :param spec: Matrix specification that contains the correct block size.
                 If block size is 1 then no reshape is performed.
        :param block_values: The list of block values to flatten.
    """
    if spec.block_size == 1:
        return block_values
    if hasattr(block_values, 'shape'):
        if len(block_values.shape) != 3 or block_values.shape[1] != spec.block_size:
            raise ValueError(f"Invalid block-shape: {block_values.shape}")
    # Need to transpose individual blocks unfortunately:
    return np.transpose(block_values, (0, 2, 1)).flatten('C')


def nestable_json_from_dict(matmul_options: dict) -> str:
    json_options = json.dumps(matmul_options)
    return json_options.replace('"', '\\\"')


def get_json_args(
        spec: MatmulSpec,
        matmul_options: dict,
        embedding_grad_scale: float = 1.0,
        dense_grad_matmul_options: dict = {}) -> str:
    # matmul_json is intended to be nested JSON that will
    # be parsed by Poplar hence has to be escaped:
    matmul_json = nestable_json_from_dict(matmul_options)
    dense_grad_matmul_json = nestable_json_from_dict(dense_grad_matmul_options)
    return "{" \
           f"\"block_size\":{spec.block_size}, " \
           f"\"batch_size\":{spec.batch_size}, " \
           f"\"input_size\":{spec.input_size}, " \
           f"\"output_size\":{spec.output_size}, " \
           f"\"max_non_zero_blocks\":{spec.max_non_zero_blocks}, " \
           f"\"group_size\":{spec.num_groups}, " \
           f"\"data_type\":\"{str(spec.data_type)}\", " \
           f"\"matmul_options\":\"{matmul_json}\", " \
           f"\"dense_grad_matmul_options\":\"{dense_grad_matmul_json}\", " \
           f"\"pooling_type\":\"{spec.pooling_type}\", " \
           f"\"embedding_grad_scale\":\"{embedding_grad_scale}\"," \
           f"\"debug_printing\":\"{int(_GLOBAL_SYSTEM_CONFIG.custom_op_debug)}\"" \
           "}"


def _get_or_create_sparse_variable(name, data_type, shape, values, constant, trainable) -> Union[tf.Tensor, tf.Variable]:
    if constant:
        if values is None:
            raise ValueError("Must pass values if variable is constant.")
        return tf.constant(values, dtype=data_type, shape=shape)
    return tf.get_variable(name, dtype=data_type, initializer=values, trainable=trainable)


def _get_or_create_nz_values(data_type, shape=None, values=None, constant=False) -> Union[tf.Tensor, tf.Variable]:
    return _get_or_create_sparse_variable("nz_values", data_type, shape, values, constant, True)


def _get_or_create_metainfo(data_type, shape, values=None, constant=False) -> Union[tf.Tensor, tf.Variable]:
    return _get_or_create_sparse_variable("metainfo", data_type, shape, values, constant, False)


def _get_or_create_dummy_dense_weights(spec: MatmulSpec) -> tf.Variable:
    # Whilst we do not want to ever instantiate the full dense weights we do need
    # a dummy variable to represent them in the Tensorflow graph that allows us
    # to request gradients with respect to them. Because this variable is never
    # read/written will be removed by Poplar's graph optimisations.
    ns = tf.get_default_graph().get_name_scope()
    if spec.pooling_type != 'NONE':
        in_size = spec.input_size // spec.block_size
        out_size = spec.output_size // spec.block_size
        logger.debug(f"Pooled dense weight shape for {ns}: {[in_size, out_size]}")
    else:
        in_size = spec.input_size
        out_size = spec.output_size
        logger.debug(f"Dense weight shape (no pooling) for {ns}: {[in_size, out_size]}")
    return tf.get_variable("dummy_dense_weights", shape=[in_size, out_size],
                           dtype=spec.data_type, trainable=False, initializer=tf.zeros_initializer())


def get_or_create_matmul_vars(
        spec: MatmulSpec,
        sparse_data: SparseRepresentation,
        matmul_options: dict,
        constant_metainfo: bool) -> Tuple[tf.Variable, tf.Variable, tf.Variable]:
    """Given a matmul spec and a host representation return variables that are
    correctly sized to store the on-device representation.

    :param spec: Matmul specification that contains shape and block size.
    :param sparse_data: Host side representation of the on-device variables.
    :param matmul_options: Options for the sparse matmul operation.
    :param constant_metainfo: Whether to turn metainfo into a constant.
    """
    metainfo_size, nz_size, splits = _get_sparse_tensor_sizes(spec, matmul_options)

    logger.info(f"Serialisation splits for dense grad W: {splits}")

    # Make vars for the representation so we can update it
    # from the host and the weights are trainable:
    trainable_nz = _get_or_create_nz_values(spec.data_type, [nz_size],
                                            sparse_data.valuesFeed(), False)
    metainfo = _get_or_create_metainfo(tf.float16, [metainfo_size],
                                       sparse_data.metaInfoFeed(), constant_metainfo)
    dummy_dense_weights = _get_or_create_dummy_dense_weights(spec)

    return trainable_nz, metainfo, dummy_dense_weights


def embedding_with_vars(
        spec: MatmulSpec,
        indices: tf.Tensor,
        matmul_options: dict,
        trainable_nz: tf.Variable,
        metainfo: tf.Variable,
        embedding_grad_scale: float) -> tf.Tensor:
    """Returns the tensor containing the embedded sequence using the variables
    from an existing sparse fully connected layer.

    :param spec: Matmul specification that contains shape and block size for the tied fc layer.
    :param indices: The tensor holding the embedding indices.
    :param matmul_options: Options for the sparse matmul operation of the tied fc layer.
    :param trainable_nz: The trainable nonzero values variable for the tied fc layer.
    :param metainfo: The metainfo variable corresponding to the tied fc layer.
    :param embedding_grad_scale: Scalar value with which to scale the gradient.
    """
    num_tokens = indices.get_shape()[0]
    result_shape = tf.TensorShape([num_tokens, spec.input_size])

    outputs = {
        "output_types": [spec.data_type],
        "output_shapes": [result_shape],
    }
    json_args = get_json_args(spec, matmul_options, embedding_grad_scale)
    inputs = [indices, metainfo, trainable_nz]

    with_grads = [2]  # No grads wanted for indices or metainfo
    return ipu.custom_ops.precompiled_user_op(
        inputs,
        library_path=get_lib_path("sparse_embedding"),
        outs=outputs,
        inputs_with_gradients=with_grads,
        attributes=json_args,
        gradient_attributes=json_args,
        separate_gradients=True)


def matmul_with_vars(
        spec: MatmulSpec,
        lhs: tf.Tensor,
        return_dense_grad: Union[bool, tf.Tensor],
        matmul_options: dict,
        trainable_nz: tf.Variable,
        metainfo: tf.Variable,
        dummy_dense_weights: tf.Variable,
        dense_grad_matmul_options: dict = {}) -> tf.Tensor:
    """Returns the tensor output of the dense * sparse matrix multiplication.

    :param spec: Matmul specification that contains shape and block size.
    :param lhs: Dense left-hand side of the matrix multiplication.
    :param return_dense_grad: Boolean to decide whether to calculate the dense grad.
    :param matmul_options: Options for the sparse matmul operation.
    :param trainable_nz: The trainable nonzero values variable.
    :param metainfo: The metainfo variable corresponding to this operation.
    :param dummy_dense_weights: The dummy variable that the dense gradient will be with respect to.
    :param dense_grad_matmul_options: Options for the dense matmul operation.
    """
    result_shape = tf.TensorShape([spec.batch_size, spec.output_size])

    outputs = {
        "output_types": [spec.data_type],
        "output_shapes": [result_shape],
    }

    json_args = get_json_args(
        spec,
        matmul_options,
        dense_grad_matmul_options=dense_grad_matmul_options)
    inputs = [lhs, metainfo, trainable_nz, return_dense_grad, dummy_dense_weights]
    with_grads = [0, 2, 4]  # No grads wanted for metainfo and scalar bool
    return ipu.custom_ops.precompiled_user_op(
        inputs,
        library_path=get_lib_path("sparse_matmul"),
        outs=outputs,
        inputs_with_gradients=with_grads,
        attributes=json_args,
        gradient_attributes=json_args)


def update_metainfo_op_with_vars(
        metainfo_ph: tf.Tensor, nz_ph: tf.Tensor,
        metainfo_var: tf.Variable, nz_var: tf.Variable) -> tf.Operation:
    """Returns an op that can be used to update the metainfo on device

    :param metainfo_ph: Metainfo placeholder
    :param nz_ph: Nonzero-values placeholder
    :param metainfo_var: Metainfo variable
    :param nz_var: Nonzero-values variable
    """
    assign_nz = nz_var.assign(nz_ph)
    assign_meta = metainfo_var.assign(metainfo_ph)

    with tf.control_dependencies([assign_nz, assign_meta]):
        update_op = tf.no_op()

    return update_op


def representation_from_triplets(
        spec: MatmulSpec,
        row_indices: List[int], col_indices: List[int], values: List[float],
        matmul_options: dict,
        *,
        debug_name: Optional[str] = "",
        n_ipus: Optional[int] = 1,
        ipu_version: Optional[int] = None) -> SparseRepresentation:
    """Returns a host-side representation corresponding to the given triplets.

    :param spec: Matmul specification that contains shape and block size.
    :param row_indices: Row indices for each of the values.
    :param col_indices: Column indices for each of the values.
    :param values: List of non-zero values.
    :param matmul_options: Options for the sparse matmul operation.
    :param debug_name: Optional debug name to pass down to the host-utils.
    :param n_ipus: Number of IPUs for the layer. (Currently not implemented.)
    :param ipu_version: Optional override of the system config value.
    """
    ipu_version = _GLOBAL_SYSTEM_CONFIG.ipu_version if ipu_version is None else ipu_version

    flat_values = flatten_blocks(spec, values)
    metainfo, nzvalues = host_utils.representation_from_triplets(
        n_ipus, ipu_version, spec, str(spec.data_type),
        row_indices, col_indices, flat_values, json.dumps(matmul_options), debug_name)
    return SparseRepresentation(metainfo, nzvalues.astype(spec.data_type.as_numpy_dtype()))


def triplets_from_representation(
        spec: MatmulSpec,
        sparse_data: SparseRepresentation,
        matmul_options: dict,
        *,
        debug_name: str = "",
        n_ipus: int = 1,
        ipu_version: int = None) -> Triplets:
    """Returns the triplets corresponding to the given host-side representation.

    :param spec: Matmul specification that contains shape and block size.
    :param sparse_data: Host-side representation.
    :param matmul_options: Options for the sparse matmul operation.
    :param debug_name: Optional debug name to pass down to the host-utils.
    :param n_ipus: Number of IPUs for the layer. (Currently not implemented.)
    :param ipu_version: Optional override of the system config value.
    """
    ipu_version = _GLOBAL_SYSTEM_CONFIG.ipu_version if ipu_version is None else ipu_version

    row_indices, col_indices, values = host_utils.triplets_from_representation(
        n_ipus, ipu_version, spec, str(spec.data_type),
        sparse_data.metainfo_state, sparse_data.nz_values, json.dumps(matmul_options), debug_name)
    return Triplets(row_indices.tolist(), col_indices.tolist(), unflatten_blocks(spec, values).tolist())


def _get_sparse_tensor_sizes(spec: MatmulSpec, matmul_options: dict, *, n_ipus: int = 1, ipu_version: int = None):
    ipu_version = _GLOBAL_SYSTEM_CONFIG.ipu_version if ipu_version is None else ipu_version
    return host_utils.get_sparse_tensor_sizes(
        n_ipus, ipu_version, spec, str(spec.data_type),
        json.dumps(matmul_options))


def triplets_from_dense(matrix: np.ndarray, block_size: int = 1) -> Triplets:
    """Given a dense matrix returns the corresponding sparse triplet representation
    by looking for exactly zero elements, or blocks that are entirely filled with
    exactly zeros in the block case.

    :param matrix: Dense numpy array.
    :param block_size: The desired block size.
    """
    if block_size == 1:
        indices = np.nonzero(matrix)
        values = matrix[indices]
        return Triplets(indices[0].tolist(), indices[1].tolist(), values.tolist())

    # Block-size > 1: test which blocks are all non-zero:
    not_divisible = [s % block_size != 0 for s in matrix.shape]
    if any(not_divisible):
        raise ValueError(f"Every dimension of matrix shape {matrix.shape} must be divisible by the block size {block_size}")
    row_blocks = matrix.shape[0] // block_size
    col_blocks = matrix.shape[1] // block_size
    row_indices = []
    col_indices = []
    non_zero_blocks = []
    for row in range(row_blocks):
        for col in range(col_blocks):
            r = row * block_size
            rend = r + block_size
            c = col * block_size
            cend = c + block_size
            block = matrix[r:rend, c:cend]
            # Check if block is zero and append block and indices if it is
            if np.count_nonzero(block) != 0:
                row_indices.append(row)
                col_indices.append(col)
                non_zero_blocks.append(block)
    return Triplets(row_indices, col_indices, non_zero_blocks)


def dense_from_triplets(spec: MatmulSpec, row_indices: List[int], col_indices: List[int], values: List[float]) -> np.ndarray:
    """Returns the dense matrix corresponding to the sparse triplets.

    :param spec: Matmul specification that contains shape and block size.
    :param row_indices: Row indices for each of the values.
    :param col_indices: Column indices for each of the values.
    :param values: List of non-zero values.
    """
    # Input is multiplied on the left in popsparse:
    dense = np.zeros(shape=[spec.input_size, spec.output_size])
    if spec.block_size == 1:
        dense[(row_indices, col_indices)] = values
        return dense

    blocks = unflatten_blocks(spec, values)
    for i, b in enumerate(blocks):
        r = row_indices[i] * spec.block_size
        rend = r + spec.block_size
        c = col_indices[i] * spec.block_size
        cend = c + spec.block_size
        dense[r:rend, c:cend] = b
    return dense


def mask_from_triplets(spec: MatmulSpec, row_indices: List[int], col_indices: List[int], values: List[float]) -> np.ndarray:
    """Returns the corresponding mask for the given sparse triplets.

    :param spec: Matmul specification that contains shape and block size.
    :param row_indices: Row indices for each of the values.
    :param col_indices: Column indices for each of the values.
    :param values: List of non-zero values.
    """
    # Input is multiplied on the left in popsparse:
    mask = np.zeros(shape=[spec.input_size, spec.output_size])
    if spec.block_size == 1:
        mask[(row_indices, col_indices)] = 1
        return mask

    block = np.ones(shape=[spec.block_size, spec.block_size])
    for i in range(len(row_indices)):
        r = int(row_indices[i] * spec.block_size)
        rend = int(r + spec.block_size)
        c = int(col_indices[i] * spec.block_size)
        cend = int(c + spec.block_size)
        mask[r:rend, c:cend] = block
    return mask


def block_mask_to_element(block_mask: np.ndarray, block_size: int):
    """
    Return the equivalent element-wise mask for a given block mask.
    """
    return np.kron(block_mask, np.ones(shape=[block_size, block_size])).astype(int)


def values_at_indices(row_indices: List[int], col_indices: List[int], matrix: np.ndarray) -> np.ndarray:
    return matrix[(row_indices, col_indices)]


def blocks_at_indices(row_indices: List[int], col_indices: List[int], block_size: int, matrix: np.ndarray):
    if block_size == 1:
        return values_at_indices(row_indices, col_indices, matrix)
    blocks = []
    for i in range(len(row_indices)):
        r = int(row_indices[i] * block_size)
        rend = int(r + block_size)
        c = int(col_indices[i] * block_size)
        cend = int(c + block_size)
        blocks.append(matrix[r:rend, c:cend])
    return blocks


def random_indices(
        spec: MatmulSpec,
        indices_initialiser_gen: Callable,
        excluded_indices: Tuple[List[int], List[int]] = None,
        count: int = None) -> np.ndarray:
    """
    Generate a random set of row and column indices according to the given matmul spec.
    :param spec: Specification for the matrix multiplication in which the sparse indices will be used.
    :param indices_initialiser_gen: Random number generator (if None, a new one is created with no seed).
    :param excluded_indices: Tuple of lists containing row and column indices that should not be
                             in the generated set.
    :param count: Number of indices to generate. Overrides the max non zeros in spec which is used
                  instead if count is None.
    """
    if indices_initialiser_gen is None:
        indices_initialiser_gen = np.random.default_rng()

    # Input is multiplied on the left in popsparse:
    block_rows = spec.input_size // spec.block_size
    block_cols = spec.output_size // spec.block_size
    number = count if count is not None else spec.max_non_zero_blocks
    logger.debug(f"Generating {number} random indices.")

    # Create a random sample of non-repeating flat indices
    # and then convert them to row, col:
    total_indices = block_rows * block_cols
    if total_indices < number:
        raise ValueError(f"Not enough indices (Attempting to draw {number} from set of {total_indices})")

    if excluded_indices is None:
        flat_indices = indices_initialiser_gen.choice(total_indices, size=number, replace=False)
    else:
        fc_shape = (block_rows, block_cols)
        excluded_flat_indices = np.ravel_multi_index(excluded_indices, fc_shape)

        # NOTE: Forming the total index list is a poor algorithm for very
        # large matrices:
        choose_from = np.delete(np.arange(total_indices), excluded_flat_indices)
        flat_indices = indices_initialiser_gen.choice(choose_from, size=number, replace=False)

    return np.unravel_index(flat_indices, (block_rows, block_cols))


def random_triplets(
        spec: MatmulSpec,
        indices_initialiser_gen: Callable,
        value_generator: Callable[..., np.ndarray],
        excluded_indices: Tuple[List[int], List[int]] = None,
        count: int = None) -> Triplets:
    """
    Generate a random set of row and column indices and values according to the given matmul spec.
    :param spec: Specification for the matrix multiplication in which the sparse indices will be used.
    :param seed: Seed for the random number generator.
    :param value_generator: Callable generator to generate the values.
    :param excluded_indices: Tuple of lists containing row and column indices that should not be
                             in the generated set.
    :param count: Number of indices to generate. Overrides the max non zeros in spec which is used
                  instead if count is None.
    """
    row_indices, col_indices = random_indices(spec, indices_initialiser_gen, excluded_indices, count)
    block_element_count = spec.block_size * spec.block_size
    values = value_generator(size=len(row_indices) * block_element_count)
    return Triplets(row_indices.tolist(), col_indices.tolist(), unflatten_blocks(spec, values).tolist())


def triplet_stats(row_indices, col_indices, values):
    """
    Compute summary statistics for row indices, column indices, and values.
    All value statistics are computed on absolute values.
    """
    abs_values = np.abs(values)
    row_range = (np.min(row_indices), np.max(row_indices))
    col_range = (np.min(col_indices), np.max(col_indices))
    value_range = (np.min(abs_values), np.max(abs_values))
    median_row_index = np.median(row_indices)
    median_col_index = np.median(col_indices)
    median_value = np.median(abs_values)
    return TripletStats(
        row_range=row_range, col_range=col_range, value_range=value_range,
        median_row_index=median_row_index,
        median_col_index=median_col_index,
        median_value=median_value
    )


def disjoint_random_indices(spec: MatmulSpec, size_a: int, size_b: int, indices_initialiser_gen: Callable) -> Tuple[List[int], List[int]]:
    """
    Return two disjoint sets of indices (A and B) for the given matrix spec
    and given set sizes. If size_a + size_b exceeds the total number of indices
    for the matrix given by spec then an exception will be raised.
    :param spec: Specification for the matrix multiplication in which the sparse indices will be used.
    :param size_a: Number of indices in first set.
    :param size_b: Number of indices in second set.
    """
    indices_a = random_indices(spec, indices_initialiser_gen, None, size_a)
    indices_b = random_indices(spec, indices_initialiser_gen, indices_a, size_b)
    return indices_a, indices_b


def gen_sparse_rand_orthog_mat(dimension, sparse_density, block_size, rng):
    if block_size != 1:
        raise ValueError(f"Block-size {block_size} is not supported.")
    N = dimension
    max_nonzero = int(np.ceil(sparse_density*N*N))

    #  initialize the sparse matrix to an identity matrix
    Q = np.identity(N)
    Q_nonZero = N
    #  loop and generate the random, orthogonal matrices and use them
    #  to construct the overall random, orthogonal matrix we wish to return
    #  until the maximum desired density is reached
    logger.info(f"begin creating sparse random orthogonal matrix with {max_nonzero} non-zeros.")
    while Q_nonZero < max_nonzero:
        # sample a random angle
        rand_u = rng.uniform(0, 1)
        theta = rand_u * 2 * math.pi

        # sample a random pair of indices
        index = rng.choice(N, size=2, replace=False)
        index.sort()

        i, j = index

        # specify the indices and values to initialize the
        # basic random, orthogonal matrix
        I = [i, i, j, j]
        J = [i, j, i, j]
        V = [math.cos(theta), -math.sin(theta), math.sin(theta), math.cos(theta)]
        Qk = np.identity(N)
        Qk[I, J] = V
        # update the overall random, orthogonal matrix
        Q = np.matmul(Qk, Q)

        Q[np.isclose(Q, np.zeros(shape=(dimension, dimension)))] = 0
        Q_nonZero = np.count_nonzero(Q)
    logger.info(f"Done creating sparse random orthogonal matrix.")

    # Confirm algorithm has produced an orthogonal matrix
    QTQ = Q.T @ Q
    QQT = Q @ Q.T
    I = np.identity(N)
    rtol = 1e-08
    atol = 1e-07
    assert np.allclose(QTQ, I, atol=atol, rtol=rtol), "Failed orthogonal check, Q.T @ Q does not equal identity"
    assert np.allclose(QQT, I, atol=atol, rtol=rtol), "Failed orthogonal check, Q @ Q.T does not equal identity"

    return Q, Q_nonZero
