# network architecture
# input shape
dtype: FLOAT16
vocab_size: 4233

# encoder related
elayers: 16
eunits: 576

# decoder related
dlayers: 1
dunits: 640         # un-used currently

# attention related
adim: 144
aheads: 4

# hybrid CTC/attention
mtlalpha: 0.0

# label smoothing
lsm_weight: 0.0

# minibatch related
batch_size: 6
fbank_size: 83
maxlen_in: 768
maxlen_tgt: 46      # sos + valid_word + eos

# optimization related
optimizer: adaml
epochs: 50
dropout_rate: 0.0

# transformer specific setting
lr: 0.8
warmup_steps: 20000
attn_dropout_rate: 0.0

# conformer specific setting
kernel_size: 31
loss_scale: 1024

# number of ipus
is_training: true
replica: 1
gradient_accumulation_count: 8

# if use ipu specific dropout
use_ipu_dropout: True

# kernel initializer's max value
initializer_range: 0.083
