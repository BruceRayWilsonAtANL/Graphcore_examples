# network architecture
# input shape
dtype: FLOAT32
vocab_size: 4233

# encoder related
elayers: 16
eunits: 576

# decoder related
dlayers: 1
dunits: 640         # un-used currently

# attention related
adim: 144
aheads: 4

# hybrid CTC/attention
mtlalpha: 0.0

# label smoothing
lsm_weight: 0.1

# minibatch related
batch_size: 1
fbank_size: 83
maxlen_in: 768
maxlen_tgt: 46      # sos + valid_word + eos

# optimization related
optimizer: adam
epochs: 50
dropout_rate: 0.1

# transformer specific setting
lr: 1.0
warmup_steps: 20000
attn_dropout_rate: 0.0

# conformer specific setting
kernel_size: 31

# number of ipus
num_ipus_per_replica: 4
is_training: true
replica: 1
global_batch_size: 32
