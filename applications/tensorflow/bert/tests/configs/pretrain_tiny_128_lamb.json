{
  "task": "pretraining",
  "attention_probs_dropout_prob": 0,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 128,
  "intermediate_size": 512,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "num_attention_heads": 2,
  "num_hidden_layers": 2,
  "use_attention_projection_bias": false,
  "use_cls_layer": false,
  "use_qkv_bias": false,
  "pipeline_stages": [
     ["emb","pos"], 
     ["hid","hid"], 
     ["mlm","nsp"]
  ],
  "device_mapping":[0,0,0],
  "type_vocab_size": 2,
  "vocab_size": 30400,
  "seq_length": 128,
  "batch_size": 8,
  "batches_per_step": 1,
  "num_train_steps": 1000,
  "epsilon": 1e-6,
  "max_predictions_per_seq": 20,
  "lr_schedule": "polynomial_decay",
  "base_learning_rate": 0.001,
  "warmup": 10,
  "loss_scaling": 1,
  "optimizer": "lamb",
  "gradient_accumulation_count": 12,
  "weight_decay_rate": 0.01,
  "parallel_io_threads": 16,
  "pipeline_schedule": "Grouped",
  "replicas": 1,
  "precision": "16",
  "seed": 1234,
  "no_outlining": false,
  "stochastic_rounding": true,
  "enable_recomputation": true,
  "fp_exceptions": false,
  "matmul_serialize_factor": 5,
  "steps_per_tensorboard": 10000,
  "steps_per_ckpts": 100,
  "steps_per_logs": 1,
  "duplicate_factor":5,

  "available_memory_proportion": 0.6,
  "partials_type": "half",
  "reduction_type": "mean",

  "init_checkpoint":"",
  "restore_dir": "",
  "save_path": "checkpoint/phase1/",
  "static_mask": true,
  "train_file": ""
}
